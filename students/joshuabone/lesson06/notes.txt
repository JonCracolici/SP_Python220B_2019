Joshua Bone - UW Python 220 - 2019-07-06

Step 1: Record the starting benchmarks for the program.

I ran gtime for the poor_perf.py file, using the 1,000,000 row CSV:
	User time (seconds): 4.71
	System time (seconds): 0.14
	Percent of CPU this job got: 99%

Step 2: Identify the bottlenecks by adding logging statements with clock times.

INFO:root:Filter years > 2012: 2.066328 secs.
INFO:root:Year counts: 0.98563 secs.
INFO:root:Count 'ao' occurrences: 1.76705 secs.

Observations:
- We process the entire CSV file twice: once to count the years, and once to
count the 'ao' occurrences.
- We do the slice logic ("new[0][6:]") six times when only once is needed.
- We build a new list of all the dates > 2012 and then iterate through it again.
- Unnecessary conversions when iterating through csv reader: 'line' is already
    a list.

Step 3: Fix observations from Step 2 (good_perf_v0.py)

INFO:root:Time elapsed: 1.801036 secs.

By looping through the CSV file only once, and being smart with type
conversions, we have improved our total runtime from 4.8 seconds to 1.8 seconds,
a 63% improvement!
